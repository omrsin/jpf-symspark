\section*{Abstract}

%The massive worldwide adoption of the Internet and the shift of many industries into the digital plane have created a gigantic platform where uncountable amounts of information are produced and shared. Processing such data in a sensible and timely manner required innovative computational paradigms; this led to the creation of several successful distributed models that proved to be better suited for tasks of such magnitude. One example is Apache Spark, a fault-tolerant distributed processing framework that uses a shared memory abstraction in order to offer better performance.

As in any other software, applications written for distributed big data frameworks are susceptible to bugs and errors. Although several program testing techniques have been ported to the context of distributed programming and have been the subject of research studies, program analysis approaches have received less attention both in the industry and the academia. Formal methods and code analyses could also prove useful towards the goal of improving code quality and their automated nature could provide a mechanism for a continuous evaluation.

This work aims to explore the applicability of model checking techniques, in particular, symbolic execution in the context of big data systems. For this purpose, it introduces \textit{JPF-SymSpark}, a symbolic execution framework for Apache Spark programs built as an extension of \acrlong{acr:jpf}. The main goal of \textit{JPF-SymSpark} is to generate reduced input datasets that offer full path coverage of the analyzed program and can be used as input data for unit tests. The tool is able to symbolically execute Spark programs that handle primitive data types and Strings as their input datasets. It is also capable of chaining multiple Spark operations during a symbolic execution; providing the mechanisms for a complete analysis of a program.

The evaluation of the approach is twofold. The first part consists of a qualitative appraisal that aims to determine how compliant \textit{JPF-SymSpark} is in terms of a series of functional and non-functional requirements defined for a tool of such purpose. Although most of the requirements are met, those that are not met represent severe obstacles to the applicability of the tool on less trivial Spark programs. The second part is a performance assessment of the iterative symbolic operations carried out during an analysis. In most cases, the performance starts degrading noticeably after thirteen iterations. Moreover, the complexity of the path conditions, the performance of the constraint solver, and the number of solvable constraints in an analysis are the major factors that affect the overall performance of iterative symbolic executions.