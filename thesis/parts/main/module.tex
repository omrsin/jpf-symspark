\TUsection{JPF-SymSpark}
\label{sec:jpf-symspark}

% Spark mocked library
% Instruction Factory
% Listener and Coordinator
% Strategies
JPF-SymSpark is a \jpf{} module whose goal is to coordinate the symbolic execution of Apache Spark programs written for its Java API to produce a reduced input dataset that ensures full path coverage on a regular execution. It builds on top of \spf{} to delegate the handling of symbolic expressions while it focuses on how to interconnect Spark's transformations and actions in order to reason coherently over the execution flow of the program. This section describes the general structure and technical aspects of the \textit{JPF-SymSpark} module. The work presented here is based on the logical processes defined in the previous section. 

The main \jpf{} extension points used in the module are:

\begin{itemize}
	\item \textbf{Bytecode Factory}: Implemented in the \textit{SparkSymbolicInstructionFactory} class, this bytecode instruction factory is in charge of detecting relevant Spark instructions.
	\item \textbf{Listeners}: The main listener is the \textit{SparkMethodListener} class. After the bytecode instruction factory, the listener provides the main interaction point with the analyzed program. In our case, the listener aims to orchestrate the correct symbolic execution of a sequence of Spark operations.
	\item \textbf{Publishers}: The \textit{SparkMethodListener} class also works as a publisher (these two responsibilities are often held together by the same class). The class produces a reduced input dataset obtained after selecting concrete values that satisfy the path conditions.
	\item \textbf{Choice Generators}: In some cases, additional choice generators must be registered in order to correctly reproduce the behavior of some of the Spark operations. Such is the case of the \textit{SparkMultipleOutputChoiceGenerator} class, which is used when analyzing a \textit{flatMap} transformation, given that it can potentially produce multiple outputs.
\end{itemize}

Additionally, the module also introduces the following control components that play an important role during the analysis:

\begin{itemize}
	\item \textbf{Validators}: These are abstractions that encapsulate the necessary validations to determine if an executed instruction is a Spark operation or an invocation of the user-defined function. As for the case of the Java platform, the validator is implemented in the \textit{JavaSparkValidator} class.
	\item \textbf{Method Strategies}: Each Spark operation follows a different strategy depending on how they deal with their input and output parameters. One example of a method strategy can be found in the \textit{FilterStrategy} class.
	\item \textbf{Method Coordinator}: The coordinator is in charge of selecting the adequate strategy based on the Spark operation currently being executed. It maintains a general view of the whole analysis. The method coordinator is implemented in the \textit{MethodSequenceCoordinator} class.
	\item \textbf{Surrogate Spark Library}: The regular Spark library undergoes several unnecessary workload that is not relevant to the symbolic execution. Instead of it, a surrogate Spark library was implemented and included as one of the dependencies of the module to relieve the analysis of the irrelevant load.
\end{itemize}

The following sections explain in more detail the different components that conform the module and what role do they play in the whole analysis.

%TODO: Maybe mention that the module was created using the jpf-template project
%TODO: Mention the structure and key aspects, for example the configuration properties, and the new Instruction Factory

\input{parts/main/spark_library}
\input{parts/main/instruction_factory}
\input{parts/main/listener}
\input{parts/main/strategies}
\input{parts/main/output}