\TUsection{Limitations}
\label{sec:limitations}

%Mention how JPF-SymSpark is limited in terms of the fixed Spark library that it checks and the Java-only compatibility
%
%The limited support to String symbolic operations. The lack of a solver that specializes on Strings makes it limited to supporting many big data tasks.
%
%Limited support to objects and how symbolic objects are created and shared
%
%The ugly ill-maintained codebase of jpf-symbc makes it cumbersome to extend and easily outdated due to abundance of code smells and bad practices.
%
%The complications when dealing with lambda expressions makes it difficult to use the tool on real spark applications. Because most current developers prefer java8 syntax favoring its flexibility and reduced verbosity.
%
%Although support for other solvers is mentioned, in practice, many of them fail due to missing libraries which are outdated when independently included.

This section comments on several limitations of \textit{JPF-SymSpark} that come as a direct consequence of the design decisions or are caused by any of the frameworks upon which the module builds up.

The processing logic of \textit{JPF-SymSpark} focuses on programs that were written complying to version~2.0.2 of the Apache Spark library. It might be the case that previous or future libraries of the tool are still compatible, however, any change in the classes mocked up by the surrogate Spark library included in the module might render those programs incompatible. Users are encouraged to modify the surrogate library to match another official Spark release as long as the semantics are preserved.

Analyses using \textit{JPF-SymSpark} are expected to be run on portions of Spark code that contain a series operations applied to a single \acrshort{acr:rdd}. Including several \acrshort{acr:rdd}s in an execution could provide an invalid outcome. This point is suggested in Section~\ref{sec:future} as one of the possible future extensions of the tool.

Furthermore, there were several concealed, or at least not evident, aspects of \spf{} that arose during the implementation of \textit{JPF-SymSpark}. These aspects represented major obstacles in the development process and had an impact on the scope of the developed tool. We consider relevant to indicate these pitfalls as part of the evaluation in order to guide future research initiatives on the field and also to improve the knowledge base when assessing \spf{} in the research context. The intent of these remarks is not to diminish \spf{} in any sense, on the contrary, it aims to guide future researchers to the weak spots that require more attention.

The most relevant impediment is the limited support of symbolic String operations. Although it has been a work in progress since 2012~\cite{Redelinghuys2012,Pasareanu2013}, there are still some key String operations that are not yet supported by \spf{}. For example, the \textit{split} operation, which is commonly used in Spark programs, is not supported and if included in an analysis it halts the verification and crashes \jpf{}. Additionally, constraints that combine conditions on the String structure and its length are not solved correctly, bypassing any restrictions set on the size of the String.

Moreover, specialized String constraint solvers are claimed to be supported, however, in practice this is no longer the case. This situation not only applies to String solvers but also to other third-party solvers specialized in more complex numerical constraints. The problem is that the implemented interfaces that communicate with the constraint solvers are outdated given that they were initially implemented based on now obsolete versions of the tools. Some solvers like CVC3~\cite{Barrett2007} are still compatible (although the newer libraries must be included), while others like Z3~\cite{DeMoura2008} are no longer compatible.

Another relevant obstacle in the context of Spark applications is the limited support of symbolic data structures, sometimes also referred to as symbolic heap or symbolic objects~\cite{Pasareanu2010}. Although supported, symbolic data structures often generate errors if used inside Spark transformations and actions. The lack of a convenient interaction mechanism when dealing with symbolic objects makes it difficult to build extensions on top of it.

Some obstacles were bested by means of a workaround. Such is the case of the lack of support of lambda expressions as target methods to be analyzed by \spf{}. As noted in a series of posts exchanged between the author and Kasper Luckow (contributor to \spf{} and author of JDart~\cite{Luckow2016}) in the official \jpf{} forum, the workaround consists in referring to the static methods in the anonymous classes that are generated as a consequence of the compilation of the lambda expression. This solution is further explained in Section~\ref{sec:contributions}.

With a few exceptions, the \jpf{} and \spf{} communities are relatively silent and the tools seem to be lacking enthusiasts. This plays a big role when trying to extend the current tools; software communities in other open-source projects have a more structured communication mechanism and a clear list of new features and bugs where the collaborators can easily share information.

Lastly, there are two aspects that have to be considered before engaging into an adaptation or extension of \spf{}: First, \spf{} has a large codebase of more than a hundred thousand lines building up for more than ten years of development. Tackling such a large codebase takes a considerable amount of time and getting used to the different programming styles makes it more challenging when identifying relevant portions of the code. Second, new versions are seldom released and when they are, they often include a huge number of undocumented changes that are not clearly specified in the revision notes. This makes it particularly hard when tracking differences between the official documentation and the current software.

%Lastly, one of the aspects that resulted to be the most cumbersome was the poor quality of the \spf{}'s source code. Frequent redundancy (for example, the \textit{IFInstrSymbHelper} class), immense amounts of commented code, even in a way that seemed to be used as a communication platform among developers, and the general lack of coding style made the extension of the tool more troublesome than what it should have been. On top of this, new revisions are seldom uploaded and when they are, they often include a huge number of undocumented changes that are not clearly specified in the revision notes. This makes it particularly hard when tracking differences between the documentation and the current software.